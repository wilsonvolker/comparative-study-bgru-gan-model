{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Model Training\n",
    "Train GAN models for HK and US stock market, repectively"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import csv\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.layers import LSTM, Conv1D, LeakyReLU, Flatten\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.metrics import Accuracy, MeanSquaredError, MeanAbsoluteError, RootMeanSquaredError, MeanAbsolutePercentageError\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Common variables and functions"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "MODEL_STRUCTURE_PATH = \"./diagrams/model_structures\"\n",
    "MODEL_TRAIN_HISTORY_DIAGRAMS_PATH = \"./diagrams/model/training\"\n",
    "PROCESSED_STOCKS_PATH = \"./data/processed/training_data\"\n",
    "# TRAINING_STOCKS_PATH = \"./data/processed/training_data\"\n",
    "EVALUATE_STOCKS_PATH = \"./data/processed/stocks_for_evaluate\"\n",
    "TRAIN_STOCK_NAMES_PATH = \"./data/processed/stock_names_for_training\"\n",
    "\n",
    "# stocks model checkpoint paths\n",
    "HK_MODELS_CHECKPOINT_PATH = \"./model/hk\"\n",
    "US_MODELS_CHECKPOINT_PATH = \"./model/us\"\n",
    "\n",
    "hk_gan_file_path = \"{}/gan.h5\".format(HK_MODELS_CHECKPOINT_PATH)\n",
    "hk_gan_train_history_file_path = \"{}/gan_training_history.npy\".format(HK_MODELS_CHECKPOINT_PATH)\n",
    "hk_gan_training_checkpoint_file_path = \"{}/ckpts\".format(HK_MODELS_CHECKPOINT_PATH)\n",
    "\n",
    "us_gan_file_path = \"{}/gan.h5\".format(US_MODELS_CHECKPOINT_PATH)\n",
    "us_gan_train_history_file_path = \"{}/gan_training_history.npy\".format(US_MODELS_CHECKPOINT_PATH)\n",
    "us_gan_training_checkpoint_file_path = \"{}/ckpts\".format(US_MODELS_CHECKPOINT_PATH)\n",
    "\n",
    "TRAIN_EPOCHS = 100\n",
    "time_lag = 30 # days (aka time steps/step size)\n",
    "CLOSE_PRICE_COLUMN_INDEX = 3 # from Data Processing.ipynb\n",
    "\n",
    "def create_dir_if_not_exist(dirname):\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname, exist_ok=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Import datasets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# # stock names\n",
    "# template_filename_train_x = \"{}/{}_train_X.npy\"\n",
    "# template_filename_train_y = \"{}/{}_train_y.npy\"\n",
    "#\n",
    "# template_filename_test_x = \"{}/{}_test_X.npy\"\n",
    "# template_filename_test_y = \"{}/{}_test_y.npy\"\n",
    "#\n",
    "# fns_hk = np.load(\"{}/hk_train_stock_names.npy\".format(TRAIN_STOCK_NAMES_PATH))\n",
    "# fns_us = np.load(\"{}/us_train_stock_names.npy\".format(TRAIN_STOCK_NAMES_PATH))\n",
    "#\n",
    "# X_train_hk = {}\n",
    "# y_train_hk = {}\n",
    "# X_test_hk = {}\n",
    "# y_test_hk = {}\n",
    "# for i in range(len(fns_hk)):\n",
    "#     X_train_hk[fns_hk[i]] = np.load(template_filename_train_x.format(\n",
    "#         TRAINING_STOCKS_PATH,\n",
    "#         fns_hk[i]\n",
    "#     ))\n",
    "#\n",
    "#     y_train_hk[fns_hk[i]] = np.load(template_filename_train_y.format(\n",
    "#         TRAINING_STOCKS_PATH,\n",
    "#         fns_hk[i]\n",
    "#     ))\n",
    "#\n",
    "#     X_test_hk[fns_hk[i]] = np.load(template_filename_test_x.format(\n",
    "#         TRAINING_STOCKS_PATH,\n",
    "#         fns_hk[i]\n",
    "#     ))\n",
    "#\n",
    "#     y_test_hk[fns_hk[i]] = np.load(template_filename_test_y.format(\n",
    "#         TRAINING_STOCKS_PATH,\n",
    "#         fns_hk[i]\n",
    "#     ))\n",
    "#\n",
    "# X_train_us = {}\n",
    "# y_train_us = {}\n",
    "# X_test_us = {}\n",
    "# y_test_us = {}\n",
    "# for i in range(len(fns_us)):\n",
    "#     X_train_us[fns_us[i]] = np.load(template_filename_train_x.format(\n",
    "#         TRAINING_STOCKS_PATH,\n",
    "#         fns_us[i]\n",
    "#     ))\n",
    "#\n",
    "#     y_train_us[fns_us[i]] = np.load(template_filename_train_y.format(\n",
    "#         TRAINING_STOCKS_PATH,\n",
    "#         fns_us[i]\n",
    "#     ))\n",
    "#\n",
    "#     X_test_us[fns_us[i]] = np.load(template_filename_test_x.format(\n",
    "#         TRAINING_STOCKS_PATH,\n",
    "#         fns_us[i]\n",
    "#     ))\n",
    "#\n",
    "#     y_test_us[fns_us[i]] = np.load(template_filename_test_y.format(\n",
    "#         TRAINING_STOCKS_PATH,\n",
    "#         fns_us[i]\n",
    "#     ))\n",
    "#\n",
    "# # Check the imports, minus the one stock that used to test generalizability\n",
    "# assert len(X_train_hk) == 49\n",
    "# assert len(y_train_hk) == 49\n",
    "# assert len(X_test_hk) == 49\n",
    "# assert len(X_test_hk) == 49\n",
    "#\n",
    "# assert len(X_train_us) == 49\n",
    "# assert len(y_train_us) == 49\n",
    "# assert len(X_test_us) == 49\n",
    "# assert len(X_test_us) == 49\n",
    "\n",
    "# hk datasets\n",
    "X_train_hk = np.load(\"{}/train_X_hk.npy\".format(PROCESSED_STOCKS_PATH))\n",
    "X_test_hk = np.load(\"{}/test_X_hk.npy\".format(PROCESSED_STOCKS_PATH))\n",
    "y_train_hk = np.load(\"{}/train_y_hk.npy\".format(PROCESSED_STOCKS_PATH))\n",
    "y_test_hk = np.load(\"{}/test_y_hk.npy\".format(PROCESSED_STOCKS_PATH))\n",
    "y_train_gan_hk = np.load(\"{}/train_y_gan_hk.npy\".format(PROCESSED_STOCKS_PATH))\n",
    "y_test_gan_hk = np.load(\"{}/test_y_gan_hk.npy\".format(PROCESSED_STOCKS_PATH))\n",
    "\n",
    "# us datasets\n",
    "X_train_us = np.load(\"{}/train_X_us.npy\".format(PROCESSED_STOCKS_PATH))\n",
    "X_test_us = np.load(\"{}/test_X_us.npy\".format(PROCESSED_STOCKS_PATH))\n",
    "y_train_us = np.load(\"{}/train_y_us.npy\".format(PROCESSED_STOCKS_PATH))\n",
    "y_test_us = np.load(\"{}/test_y_us.npy\".format(PROCESSED_STOCKS_PATH))\n",
    "y_train_gan_us = np.load(\"{}/train_y_gan_us.npy\".format(PROCESSED_STOCKS_PATH))\n",
    "y_test_gan_us = np.load(\"{}/test_y_gan_us.npy\".format(PROCESSED_STOCKS_PATH))\n",
    "\n",
    "# Check the imports\n",
    "assert X_train_hk is not None\n",
    "assert X_test_hk is not None\n",
    "assert y_train_hk is not None\n",
    "assert y_test_hk is not None\n",
    "assert y_train_gan_hk is not None\n",
    "assert y_test_gan_hk is not None\n",
    "\n",
    "assert X_train_us is not None\n",
    "assert X_test_us is not None\n",
    "assert y_train_us is not None\n",
    "assert y_test_us is not None\n",
    "assert y_train_gan_us is not None\n",
    "assert y_test_gan_us is not None"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Define models structure\n",
    "##### GAN models\n",
    "###### Reference:\n",
    "```\n",
    "Zhou, X., Pan, Z., Hu, G., Tang, S., & Zhao, C. (2018). Stock market prediction on high-frequency data using generative adversarial nets. Mathematical Problems in Engineering, 2018.\n",
    "Salimath, S., Chatterjee, T., Mathai, T., Kamble, P., & Kolhekar, M. (2021, April). Prediction of Stock Price for Indian Stock Market: A Comparative Study Using LSTM and GRU. In International Conference on Advances in Computing and Data Sciences (pp. 292-302). Springer, Cham.\n",
    "Lin, H., Chen, C., Huang, G., & Jafari, A. (2021). Stock price prediction using Generative Adversarial Networks. Journal of Computer Science, (17(3), 188–196. doi:10.3844/jcssp.2021.188.196\n",
    "https://github.com/grudloff/stock_market_GAN\n",
    "https://github.com/yiweizhang526/time-series-prediction-with-gan\n",
    "Train with multiple stocks: https://www.kaggle.com/humamfauzi/multiple-stock-prediction-using-single-nn\n",
    "Tensorflow doc: https://www.tensorflow.org/tutorials/generative/dcgan\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### GAN - Generator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_generator(input_dim, feature_cnt) -> tf.keras.models.Model:\n",
    "    model = Sequential()\n",
    "\n",
    "    # Input layer\n",
    "    model.add(\n",
    "        Input(\n",
    "            shape=(input_dim, feature_cnt)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # First layer LSTM + dropout layer\n",
    "    model.add(\n",
    "        LSTM(\n",
    "            units=128,\n",
    "            return_sequences=True,\n",
    "            activation=\"tanh\",\n",
    "            input_shape=(input_dim, feature_cnt)\n",
    "        )\n",
    "    )\n",
    "    model.add(\n",
    "        Dropout(rate=0.3)\n",
    "    )\n",
    "\n",
    "    # Second layer LSTM + dropout layer\n",
    "    model.add(\n",
    "        LSTM(\n",
    "            units=128,\n",
    "            return_sequences=False,\n",
    "            activation=\"tanh\",\n",
    "            input_shape=(input_dim, feature_cnt)\n",
    "        )\n",
    "    )\n",
    "    model.add(\n",
    "        Dropout(rate=0.5)\n",
    "    )\n",
    "\n",
    "    # Output dense layer with relu\n",
    "    model.add(\n",
    "        Dense(\n",
    "            units=1,\n",
    "            activation=\"relu\",\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # compile model and use Adam optimizer\n",
    "    model.compile(\n",
    "        optimizer=Adam(learning_rate=0.0001),\n",
    "        loss=None,\n",
    "        metrics=[\n",
    "            MeanSquaredError(),\n",
    "            MeanAbsoluteError(),\n",
    "            RootMeanSquaredError(),\n",
    "            MeanAbsolutePercentageError()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(model.summary())\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "###### GAN - Discriminator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def make_discriminator(feature_cnt) -> tf.keras.models.Model:\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(\n",
    "        Input(\n",
    "            shape=(time_lag + 1, 1) # https://github.com/yiweizhang526/time-series-prediction-with-gan/blob/d09e5eecca8e85beeea88bf331d35cfc7614a223/keras_code/keras_GAN.py#L82\n",
    "            # shape=((time_lag + 1), feature_cnt)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 1st cnn\n",
    "    model.add(\n",
    "        Conv1D(32,\n",
    "               input_shape=(time_lag + 1, 1),\n",
    "               # input_shape=((time_lag + 1), feature_cnt),\n",
    "               kernel_size=3,\n",
    "               strides=2,\n",
    "               padding=\"same\",\n",
    "               activation=LeakyReLU(alpha=0.01)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # 2nd cnn + bn\n",
    "    model.add(\n",
    "        Conv1D(64, kernel_size=5, strides=2, padding=\"same\", activation=LeakyReLU(alpha=0.01))\n",
    "    )\n",
    "    model.add(\n",
    "        BatchNormalization()\n",
    "    )\n",
    "\n",
    "    # 3rd cnn + bn\n",
    "    model.add(\n",
    "        Conv1D(128, kernel_size=5, strides=2, padding=\"same\", activation=LeakyReLU(alpha=0.01))\n",
    "    )\n",
    "    model.add(\n",
    "        BatchNormalization()\n",
    "    )\n",
    "\n",
    "    model.add(\n",
    "        Flatten()\n",
    "    )\n",
    "\n",
    "    model.add(\n",
    "        Dense(128, activation=LeakyReLU(), use_bias=False)\n",
    "    )\n",
    "\n",
    "    model.add(\n",
    "        # Dense(2, activation=\"sigmoid\")\n",
    "        Dense(1, activation=\"sigmoid\")\n",
    "    )\n",
    "\n",
    "    print(model.summary())\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot model structure (Generator)\n",
    "create_dir_if_not_exist(MODEL_STRUCTURE_PATH)\n",
    "plot_model(\n",
    "    # make_generator(input_dim=X_train_hk[fns_hk[0]].shape[1],\n",
    "    #                 feature_cnt=X_train_hk[fns_hk[0]].shape[2]\n",
    "    #                 ),\n",
    "    make_generator(input_dim=X_train_hk.shape[1],\n",
    "                    feature_cnt=X_train_hk.shape[2]\n",
    "                    ),\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    to_file=\"{}/GAN_generator_structure.png\".format(MODEL_STRUCTURE_PATH)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# plot model structure (Discriminator)\n",
    "create_dir_if_not_exist(MODEL_STRUCTURE_PATH)\n",
    "plot_model(\n",
    "    make_discriminator(feature_cnt=X_train_hk.shape[2]),\n",
    "    show_shapes=True,\n",
    "    show_layer_names=True,\n",
    "    to_file=\"{}/GAN_generator_structure.png\".format(MODEL_STRUCTURE_PATH)\n",
    ")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Model training"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "create_dir_if_not_exist(HK_MODELS_CHECKPOINT_PATH)\n",
    "create_dir_if_not_exist(US_MODELS_CHECKPOINT_PATH)\n",
    "create_dir_if_not_exist(hk_gan_training_checkpoint_file_path)\n",
    "create_dir_if_not_exist(us_gan_training_checkpoint_file_path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# ref:\n",
    "# Lin, H., Chen, C., Huang, G., & Jafari, A. (2021). Stock price prediction using Generative Adversarial Networks. Journal of Computer Science, (17(3), 188–196. doi:10.3844/jcssp.2021.188.196\n",
    "# github: https://github.com/hungchun-lin/Stock-price-prediction-using-GAN\n",
    "# Tensorflow doc:\n",
    "# https://www.tensorflow.org/tutorials/generative/dcgan\n",
    "class GAN:\n",
    "    def __init__(self,\n",
    "                 generator: tf.keras.models.Model,\n",
    "                 discriminator: tf.keras.models.Model\n",
    "                 ):\n",
    "        self.generator: tf.keras.models.Model = generator\n",
    "        self.discriminator: tf.keras.models.Model = discriminator\n",
    "        # self.generator_optimizer = Adam(0.00016) # from Lin et.al (2016)\n",
    "        self.generator_optimizer = SGD(0.0004)\n",
    "        # self.discriminator_optimizer = Adam(0.00016) # from Lin et.al (2016)\n",
    "        self.discriminator_optimizer = SGD(0.02)\n",
    "        self.checkpoint = tf.train.Checkpoint(\n",
    "            generator_optimizer=self.generator_optimizer,\n",
    "            discriminator_optimizer=self.discriminator_optimizer,\n",
    "            generator=self.generator,\n",
    "            discriminator=self.discriminator,\n",
    "        )\n",
    "\n",
    "    def generator_loss(self, d_output_fake):\n",
    "        # use tf.ones_like(...) to decrease the forecast error loss in generator\n",
    "        g_loss = binary_crossentropy(\n",
    "            tf.ones_like(d_output_fake),\n",
    "            d_output_fake,\n",
    "            from_logits=True\n",
    "        )\n",
    "\n",
    "        return g_loss\n",
    "\n",
    "    def discriminator_loss(self, d_output_real, d_output_fake):\n",
    "        # \"Classifying Real Y into class 1 and Fake Y into class 0\",\n",
    "        #   said Zhou et.al. (2018). Stock market prediction on high-frequency data using generative adversarial nets.\n",
    "        real_loss = binary_crossentropy(\n",
    "            tf.ones_like(d_output_real), # to confuse the discriminator\n",
    "            d_output_real,\n",
    "            from_logits=True\n",
    "        )\n",
    "        fake_loss = binary_crossentropy(\n",
    "            tf.zeros_like(d_output_fake), # to confuse the discriminator\n",
    "            d_output_fake,\n",
    "            from_logits=True\n",
    "        )\n",
    "        d_loss = real_loss + fake_loss\n",
    "        return d_loss\n",
    "\n",
    "    @tf.function # compile the method\n",
    "    def train_step(self, real_x, real_y, real_y_gan):\n",
    "        with tf.GradientTape() as gen_tape, tf.GradientTape() as disc_tape:\n",
    "            # get generated y values\n",
    "            generated_y = generator(real_x, training=True)\n",
    "\n",
    "            # reshape the data\n",
    "            real_y_reshaped = tf.reshape(real_y, [real_y.shape[0], real_y.shape[1], 1])\n",
    "            generated_y_reshaped = tf.reshape(generated_y, [generated_y.shape[0], generated_y.shape[1], 1])\n",
    "\n",
    "            # concat the y values data into: y_t-30, y_t-29, y_t-28, ..., y_t-1, ^y_t or y_t\n",
    "            #  where teh y_t-n is from the real data, and the y_t is generated/obtained from LSTM/training data, respectively\n",
    "            d_data_real = tf.concat([real_y_reshaped, real_y_gan], axis=1)\n",
    "            d_data_fake = tf.concat([tf.cast(generated_y_reshaped, tf.float64), real_y_gan], axis=1)\n",
    "\n",
    "            # feed the data into the discriminator, in sequence: 1. real, 2. fake\n",
    "            d_output_real = self.discriminator(d_data_real, training=True)\n",
    "            d_output_fake = self.discriminator(d_data_fake, training=False)\n",
    "\n",
    "            # calc the loss of respective models\n",
    "            g_loss = self.generator_loss(d_output_fake)\n",
    "            d_loss = self.discriminator_loss(d_output_real, d_output_fake)\n",
    "\n",
    "        gradients_of_generator = gen_tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        gradients_of_discriminator = disc_tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "\n",
    "        self.generator_optimizer.apply_gradients(\n",
    "            zip(\n",
    "                gradients_of_generator,\n",
    "                self.generator.trainable_variables\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.discriminator_optimizer.apply_gradients(\n",
    "            zip(\n",
    "                gradients_of_discriminator,\n",
    "                self.discriminator.trainable_variables\n",
    "            )\n",
    "        )\n",
    "\n",
    "        return real_y, generated_y, {'d_loss': d_loss, 'g_loss': g_loss}\n",
    "\n",
    "    def train(self, real_x, real_y, real_y_gan, model_save_path, checkpoint_save_path):\n",
    "        train_history = {}\n",
    "        train_history[\"d_loss\"] = []\n",
    "        train_history[\"g_loss\"] = []\n",
    "        train_history[\"epoch\"] = []\n",
    "\n",
    "        real_price = []\n",
    "        predicted_price = []\n",
    "\n",
    "        for epoch in range(TRAIN_EPOCHS):\n",
    "            # do train\n",
    "            tmp_real_price, tmp_fake_price, tmp_loss = self.train_step(\n",
    "                real_x = real_x,\n",
    "                real_y = real_y,\n",
    "                real_y_gan = real_y_gan\n",
    "            )\n",
    "\n",
    "            # save to history lists\n",
    "            train_history[\"d_loss\"].append(tmp_loss[\"d_loss\"].numpy()) # use .numpy() to convert from Tensor to numpy nd-array\n",
    "            train_history[\"g_loss\"].append(tmp_loss[\"g_loss\"].numpy())\n",
    "            train_history[\"epoch\"].append(epoch)\n",
    "\n",
    "            real_price.append(tmp_real_price.numpy())\n",
    "            predicted_price.append(tmp_fake_price.numpy())\n",
    "\n",
    "            # Save model per 15 epochs\n",
    "            if (epoch + 1) % 15 == 0:\n",
    "                tf.keras.models.save_model(self.generator, model_save_path)\n",
    "                self.checkpoint.save(file_prefix=checkpoint_save_path)\n",
    "                \n",
    "            print(\"\\nEpoch: {}\".format(epoch+1))\n",
    "            print(\"Discriminator loss: {} - Generator loss: {} -\".format(\n",
    "                tmp_loss[\"d_loss\"].numpy(),\n",
    "                tmp_loss[\"g_loss\"].numpy()\n",
    "            ))\n",
    "            print(\"Generator model saved.\")\n",
    "\n",
    "        # Reshape the predicted & real results\n",
    "        predicted_price = np.array(predicted_price)\n",
    "        predicted_price = predicted_price.reshape(\n",
    "            predicted_price.shape[1],\n",
    "            predicted_price.shape[2]\n",
    "        )\n",
    "\n",
    "        real_price = np.array(real_price)\n",
    "        real_price = real_price.reshape(\n",
    "            real_price.shape[1],\n",
    "            real_price.shape[2]\n",
    "        )\n",
    "\n",
    "        return predicted_price, real_price, train_history, self.generator"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Train BGRU model for Hong Kong Stocks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# define HK model's generator and discriminator\n",
    "generator = make_generator(\n",
    "    input_dim=X_train_hk.shape[1],\n",
    "    feature_cnt=X_train_hk.shape[2]\n",
    ")\n",
    "\n",
    "discriminator = make_discriminator(\n",
    "    feature_cnt=X_train_hk.shape[2]\n",
    ")\n",
    "\n",
    "hk_gan = GAN(generator, discriminator)\n",
    "hk_gan_predicted_y, hk_gan_real_y, hk_gan_history, hk_gan_model = hk_gan.train(\n",
    "    real_x=X_train_hk,\n",
    "    real_y=y_train_hk,\n",
    "    real_y_gan=y_train_gan_hk,\n",
    "    model_save_path=hk_gan_file_path,\n",
    "    checkpoint_save_path=hk_gan_training_checkpoint_file_path,\n",
    ")\n",
    "\n",
    "if hk_gan_model is not None:\n",
    "    hk_gan_model.save(hk_gan_file_path)\n",
    "    print(\"Model Saved\")\n",
    "\n",
    "if hk_gan_history is not None:\n",
    "    np.save(hk_gan_train_history_file_path, hk_gan_history)\n",
    "    # np.save(hk_bgru_train_history_file_path, hk_bgru_history)\n",
    "    print(\"History Saved\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "##### Train BGRU model for United States stocks"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n",
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### plot training history"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def plot_history(history_dict, title):\n",
    "    \"\"\"\n",
    "    Plot the training history\n",
    "    :param history_dict: dict, the training history, should be a dict (from keras' history.history)\n",
    "    :param title: str, plot title, example: \"HK BGRU Model - {}\", the program will replace the {} with the relevant metric name\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    # metrics = [\"loss\",\n",
    "    #        \"mean_absolute_error\",\n",
    "    #        \"root_mean_squared_error\",\n",
    "    #        # \"mean_absolute_percentage_error\" # disable plot of MAPE as the normalized data consists of 0 or nearly 0, the MAPE is unreasonably high, will recalculate in evaluation stage\n",
    "    #        # \"val_loss\",\n",
    "    #        # \"val_mean_absolute_error\",\n",
    "    #        # \"val_root_mean_squared_error\",\n",
    "    #        # \"val_mean_absolute_percentage_error\"\n",
    "    #        ]\n",
    "\n",
    "    # for metric in metrics:\n",
    "    plt.figure(figsize=(14, 5), dpi=500, facecolor=\"white\")\n",
    "    # metrics.replace(\"_\", \"\").title()\n",
    "\n",
    "    # parse as float\n",
    "    history_dict[\"d_loss\"] = list(map(float, history_dict[\"d_loss\"]))\n",
    "    history_dict[\"g_loss\"] = list(map(float, history_dict[\"g_loss\"]))\n",
    "\n",
    "    plt.plot(history_dict[\"d_loss\"], label=\"D_loss\")\n",
    "    plt.plot(history_dict[\"g_loss\"], label=\"G_loss\")\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel('Loss (Binary cross-entropy)')\n",
    "    plt.title(title.format(\"Loss (Binary cross-entropy)\"))\n",
    "    plt.legend()\n",
    "    create_dir_if_not_exist(MODEL_TRAIN_HISTORY_DIAGRAMS_PATH)\n",
    "    # plt.savefig('{}/{}.png'.format(MODEL_TRAIN_HISTORY_DIAGRAMS_PATH, plt.gca().get_title()))\n",
    "    plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def read_his_file(filepath, is_csv = False) -> dict:\n",
    "    result = None\n",
    "    if is_csv:\n",
    "        # read from csv and convert to dict\n",
    "        reader = csv.DictReader(open(filepath))\n",
    "\n",
    "        result = {}\n",
    "        for row in reader:\n",
    "            for column, value in row.items():  # consider .iteritems() for Python 2\n",
    "                result.setdefault(column, []).append(value)\n",
    "\n",
    "    else:\n",
    "        result = np.load(filepath, allow_pickle=True).item()\n",
    "\n",
    "    return result"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "hk_gan_history_dict = read_his_file(hk_gan_train_history_file_path)\n",
    "\n",
    "plot_history(hk_gan_history_dict, \"GAN Model for HK Stock Price Predictions - {}\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}