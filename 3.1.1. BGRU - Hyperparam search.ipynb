{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SG4y-bi3GyqV"
   },
   "source": [
    "#### Setup for Google Colab (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 37242,
     "status": "ok",
     "timestamp": 1640514845271,
     "user": {
      "displayName": "Wilson Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12240555497150083704"
     },
     "user_tz": -480
    },
    "id": "0Xmi3AJgGyqX",
    "outputId": "d8caaddd-b44f-4ee4-ffe3-936470a20be5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-b80391d2b2cf>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/drive'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWlYdoF5HdRH"
   },
   "source": [
    "##### UPDATE IT IF NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 611,
     "status": "ok",
     "timestamp": 1640514848596,
     "user": {
      "displayName": "Wilson Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12240555497150083704"
     },
     "user_tz": -480
    },
    "id": "Q1Od0SluGyqY",
    "outputId": "46d11251-5a09-4674-8202-6286d303f6d9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "cd 'drive/MyDrive/Colab Notebooks/comparative-study-bgru-gan-model'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 738,
     "status": "ok",
     "timestamp": 1640514850722,
     "user": {
      "displayName": "Wilson Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12240555497150083704"
     },
     "user_tz": -480
    },
    "id": "5TpFcWnXGyqY",
    "outputId": "26370253-7a9d-478e-eeb5-671bea9d38fd",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "ls"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZefRgBrZGyqY",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Hyper parameter searching with keras-tuner\n",
    "Train BGRU models for HK and US stock market, repectively\n",
    "<p>\n",
    "ref: https://www.tensorflow.org/tutorials/keras/keras_tuner\n",
    "\n",
    "```bash\n",
    "conda install keras-tuner\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 478,
     "status": "ok",
     "timestamp": 1640610539432,
     "user": {
      "displayName": "Wilson Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12240555497150083704"
     },
     "user_tz": -480
    },
    "id": "GHDFK7fp1SK3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.layers import GRU\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.metrics import Accuracy, MeanAbsoluteError, RootMeanSquaredError, MeanAbsolutePercentageError, MeanSquaredError\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cnwcBKNd1SK8",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Common variables and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 332,
     "status": "ok",
     "timestamp": 1640615114662,
     "user": {
      "displayName": "Wilson Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12240555497150083704"
     },
     "user_tz": -480
    },
    "id": "PXJCbw501SK9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "MODEL_STRUCTURE_PATH = \"./diagrams/model/structures\"\n",
    "MODEL_TRAIN_HISTORY_DIAGRAMS_PATH = \"./diagrams/model/training\"\n",
    "PROCESSED_STOCKS_PATH = \"./data/processed/training_data\"\n",
    "TRAINING_STOCKS_PATH = \"./data/processed/training_data\"\n",
    "EVALUATE_STOCKS_PATH = \"./data/processed/stocks_for_evaluate\"\n",
    "TRAIN_STOCK_NAMES_PATH = \"./data/processed/stock_names_for_training\"\n",
    "\n",
    "# stocks model checkpoint paths\n",
    "HK_MODELS_CHECKPOINT_PATH = \"./model/hk\"\n",
    "US_MODELS_CHECKPOINT_PATH = \"./model/us\"\n",
    "\n",
    "hk_bgru_file_path = \"{}/bgru.h5\".format(HK_MODELS_CHECKPOINT_PATH)\n",
    "hk_bgru_train_history_file_path = \"{}/bgru_training_history.npy\".format(HK_MODELS_CHECKPOINT_PATH)\n",
    "# hk_bgru_train_history_file_path = \"{}/bgru_history/{}_bgru_training_history.npy\"\n",
    "\n",
    "us_bgru_file_path = \"{}/bgru.h5\".format(US_MODELS_CHECKPOINT_PATH)\n",
    "us_bgru_train_history_file_path = \"{}/bgru_training_history.npy\".format(US_MODELS_CHECKPOINT_PATH)\n",
    "\n",
    "TRAIN_EPOCHS = 100\n",
    "\n",
    "def create_dir_if_not_exist(dirname):\n",
    "    if not os.path.exists(dirname):\n",
    "        os.makedirs(dirname, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJBSBRnF1SK_",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 5694,
     "status": "ok",
     "timestamp": 1640610545122,
     "user": {
      "displayName": "Wilson Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12240555497150083704"
     },
     "user_tz": -480
    },
    "id": "FWNVI-Qo1SLA",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# stock names\n",
    "# template_filename_train_x = \"{}/{}_train_X.npy\"\n",
    "# template_filename_train_y = \"{}/{}_train_y.npy\"\n",
    "#\n",
    "# template_filename_test_x = \"{}/{}_test_X.npy\"\n",
    "# template_filename_test_y = \"{}/{}_test_y.npy\"\n",
    "#\n",
    "# fns_hk = np.load(\"{}/hk_train_stock_names.npy\".format(TRAIN_STOCK_NAMES_PATH))\n",
    "# fns_us = np.load(\"{}/us_train_stock_names.npy\".format(TRAIN_STOCK_NAMES_PATH))\n",
    "#\n",
    "# X_train_hk = {}\n",
    "# y_train_hk = {}\n",
    "# X_test_hk = {}\n",
    "# y_test_hk = {}\n",
    "# for i in range(len(fns_hk)):\n",
    "#     X_train_hk[fns_hk[i]] = np.load(template_filename_train_x.format(\n",
    "#         TRAINING_STOCKS_PATH,\n",
    "#         fns_hk[i]\n",
    "#     ))\n",
    "#\n",
    "#     y_train_hk[fns_hk[i]] = np.load(template_filename_train_y.format(\n",
    "#         TRAINING_STOCKS_PATH,\n",
    "#         fns_hk[i]\n",
    "#     ))\n",
    "#\n",
    "#     X_test_hk[fns_hk[i]] = np.load(template_filename_test_x.format(\n",
    "#         TRAINING_STOCKS_PATH,\n",
    "#         fns_hk[i]\n",
    "#     ))\n",
    "#\n",
    "#     y_test_hk[fns_hk[i]] = np.load(template_filename_test_y.format(\n",
    "#         TRAINING_STOCKS_PATH,\n",
    "#         fns_hk[i]\n",
    "#     ))\n",
    "#\n",
    "# X_train_us = {}\n",
    "# y_train_us = {}\n",
    "# X_test_us = {}\n",
    "# y_test_us = {}\n",
    "# for i in range(len(fns_us)):\n",
    "#     X_train_us[fns_us[i]] = np.load(template_filename_train_x.format(\n",
    "#         TRAINING_STOCKS_PATH,\n",
    "#         fns_us[i]\n",
    "#     ))\n",
    "#\n",
    "#     y_train_us[fns_us[i]] = np.load(template_filename_train_y.format(\n",
    "#         TRAINING_STOCKS_PATH,\n",
    "#         fns_us[i]\n",
    "#     ))\n",
    "#\n",
    "#     X_test_us[fns_us[i]] = np.load(template_filename_test_x.format(\n",
    "#         TRAINING_STOCKS_PATH,\n",
    "#         fns_us[i]\n",
    "#     ))\n",
    "#\n",
    "#     y_test_us[fns_us[i]] = np.load(template_filename_test_y.format(\n",
    "#         TRAINING_STOCKS_PATH,\n",
    "#         fns_us[i]\n",
    "#     ))\n",
    "#\n",
    "# # Check the imports, minus the one stock that used to test generalizability\n",
    "# assert len(X_train_hk) == 49\n",
    "# assert len(y_train_hk) == 49\n",
    "# assert len(X_test_hk) == 49\n",
    "# assert len(X_test_hk) == 49\n",
    "#\n",
    "# assert len(X_train_us) == 49\n",
    "# assert len(y_train_us) == 49\n",
    "# assert len(X_test_us) == 49\n",
    "# assert len(X_test_us) == 49\n",
    "\n",
    "# hk datasets\n",
    "X_train_hk = np.load(\"{}/train_X_hk.npy\".format(PROCESSED_STOCKS_PATH))\n",
    "X_test_hk = np.load(\"{}/test_X_hk.npy\".format(PROCESSED_STOCKS_PATH))\n",
    "y_train_hk = np.load(\"{}/train_y_hk.npy\".format(PROCESSED_STOCKS_PATH))\n",
    "y_test_hk = np.load(\"{}/test_y_hk.npy\".format(PROCESSED_STOCKS_PATH))\n",
    "\n",
    "# us datasets\n",
    "X_train_us = np.load(\"{}/train_X_us.npy\".format(PROCESSED_STOCKS_PATH))\n",
    "X_test_us = np.load(\"{}/test_X_us.npy\".format(PROCESSED_STOCKS_PATH))\n",
    "y_train_us = np.load(\"{}/train_y_us.npy\".format(PROCESSED_STOCKS_PATH))\n",
    "y_test_us = np.load(\"{}/test_y_us.npy\".format(PROCESSED_STOCKS_PATH))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sK7uZlmj1SLB",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Define models structure\n",
    "##### BGRU models\n",
    "###### Reference:\n",
    "```\n",
    "Salimath, S., Chatterjee, T., Mathai, T., Kamble, P., & Kolhekar, M. (2021, April). Prediction of Stock Price for Indian Stock Market: A Comparative Study Using LSTM and GRU. In International Conference on Advances in Computing and Data Sciences (pp. 292-302). Springer, Cham.\n",
    "Lin, H., Chen, C., Huang, G., & Jafari, A. (2021). Stock price prediction using Generative Adversarial Networks. Journal of Computer Science, (17(3), 188–196. doi:10.3844/jcssp.2021.188.196\n",
    "https://github.com/grudloff/stock_market_GAN\n",
    "Train with multiple stocks: https://www.kaggle.com/humamfauzi/multiple-stock-prediction-using-single-nn\n",
    "Priya, R. S., & Sruthi, C. (2022). Stock Price Prediction Based on Deep Learning Using Long Short-Term Memory. In Futuristic Communication and Network Technologies (pp. 67-76). Springer, Singapore.\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1640610545123,
     "user": {
      "displayName": "Wilson Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12240555497150083704"
     },
     "user_tz": -480
    },
    "id": "lgSxMyoq1SLC",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "input_dim = X_train_hk.shape[1]\n",
    "feature_cnt = X_train_hk.shape[2]\n",
    "def make_bgru_model(hp) -> tf.keras.models.Model:\n",
    "    model = Sequential()\n",
    "\n",
    "    # input layer\n",
    "    model.add(\n",
    "        Input(\n",
    "            shape=(input_dim, feature_cnt)\n",
    "        )\n",
    "    )\n",
    "\n",
    "    bgru_layer1_units = hp.Int('bgru_layer1_units', min_value=32, max_value=512, step=32)\n",
    "    bgru_dropout1_rate = hp.Float(\"bgru_dropout1_rate\", min_value=0, max_value=0.99, step=0.05)\n",
    "    # first gru + dropout layer\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            GRU(\n",
    "                units=bgru_layer1_units,\n",
    "                return_sequences=True,\n",
    "                input_shape=(input_dim, feature_cnt),\n",
    "                activation=\"tanh\"\n",
    "                )\n",
    "        )\n",
    "    )\n",
    "    model.add(\n",
    "        # Dropout(rate=0.3)\n",
    "        Dropout(rate=bgru_dropout1_rate)\n",
    "    )\n",
    "\n",
    "    bgru_layer2_units = hp.Int('bgru_layer2_units', min_value=32, max_value=512, step=32)\n",
    "    bgru_dropout2_rate = hp.Float(\"bgru_dropout2_rate\", min_value=0, max_value=0.99, step=0.05)\n",
    "    # second gru + dropout layer\n",
    "    model.add(\n",
    "        Bidirectional(\n",
    "            GRU(\n",
    "                # units=64,\n",
    "                units=bgru_layer2_units,\n",
    "                return_sequences=False, # important, convert array from 3d to 2d\n",
    "                input_shape=(input_dim, feature_cnt),\n",
    "                activation=\"tanh\"\n",
    "                )\n",
    "        )\n",
    "    )\n",
    "    model.add(\n",
    "        # Dropout(rate=0.5)\n",
    "        Dropout(rate=bgru_dropout2_rate)\n",
    "    )\n",
    "\n",
    "    # output dense layer\n",
    "    model.add(\n",
    "        Dense(units = 1)\n",
    "    )\n",
    "\n",
    "    hp_adam_lr = hp.Choice('hp_adam_lr', values=[1e-2, 1e-3, 1e-4, 1e-5])\n",
    "    # compile model and use Adam optimizer\n",
    "    model.compile(\n",
    "        # optimizer=Adam(learning_rate=0.001),\n",
    "        optimizer=Adam(learning_rate=hp_adam_lr),\n",
    "        loss=\"mean_squared_error\",\n",
    "        metrics=[\n",
    "            MeanAbsoluteError(),\n",
    "            RootMeanSquaredError(),\n",
    "            MeanAbsolutePercentageError()\n",
    "        ]\n",
    "    )\n",
    "\n",
    "    print(model.summary())\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start searching the model hyper-parameter, take HK model as example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 28 Complete [00h 13m 19s]\n",
      "val_loss: 0.006776556838303804\n",
      "\n",
      "Best val_loss So Far: 0.001608289428986609\n",
      "Total elapsed time: 04h 46m 51s\n",
      "\n",
      "Search: Running Trial #29\n",
      "\n",
      "Hyperparameter    |Value             |Best Value So Far \n",
      "bgru_layer1_units |416               |192               \n",
      "bgru_dropout1_rate|0.95              |0.1               \n",
      "bgru_layer2_units |288               |320               \n",
      "bgru_dropout2_rate|0.65              |0.3               \n",
      "hp_adam_lr        |0.001             |0.001             \n",
      "tuner/epochs      |2                 |2                 \n",
      "tuner/initial_e...|0                 |0                 \n",
      "tuner/bracket     |4                 |4                 \n",
      "tuner/round       |0                 |0                 \n",
      "\n",
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "bidirectional (Bidirectional (None, 30, 832)           1065792   \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 30, 832)           0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 576)               1938816   \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 576)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 577       \n",
      "=================================================================\n",
      "Total params: 3,005,185\n",
      "Trainable params: 3,005,185\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/2\n",
      " 947/2982 [========>.....................] - ETA: 5:06 - loss: 0.0412 - mean_absolute_error: 0.1346 - root_mean_squared_error: 0.2029 - mean_absolute_percentage_error: 105.3027"
     ]
    }
   ],
   "source": [
    "tuner = kt.Hyperband(\n",
    "    make_bgru_model,\n",
    "    objective='val_loss',\n",
    "    max_epochs=TRAIN_EPOCHS,\n",
    "    factor=3,\n",
    "    directory=\"model_hyperparam_search\",\n",
    "    project_name='bgru_hyperparam_search'\n",
    ")\n",
    "\n",
    "hk_bgru_es = EarlyStopping(\n",
    "    monitor=\"val_loss\",\n",
    "    mode=\"min\",\n",
    "    patience=5,\n",
    "    # min_delta=0.0001, # https://stackoverflow.com/a/63495687/9500852\n",
    ")\n",
    "\n",
    "tuner.search(\n",
    "    X_train_hk,\n",
    "    y_train_hk,\n",
    "    validation_data=(X_test_hk, y_test_hk)\n",
    ")\n",
    "\n",
    "best_hps = tuner.get_best_hyperparameters(num_trails=1)[0]\n",
    "\n",
    "print(f\"\"\"\n",
    "The hyperparameter search is complete. \\n\n",
    "bgru_layer1_units: {best_hps.get('bgru_layer1_units')} \\n\n",
    "bgru_dropout1_rate: {best_hps.get('bgru_dropout1_rate')} \\n\n",
    "bgru_layer2_units: {best_hps.get('bgru_layer2_units')} \\n\n",
    "bgru_dropout2_rate: {best_hps.get('bgru_dropout2_rate')} \\n\n",
    "hp_adam_lr: {best_hps.get('hp_adam_lr')} \\n\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 4533799,
     "status": "error",
     "timestamp": 1640615080371,
     "user": {
      "displayName": "Wilson Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12240555497150083704"
     },
     "user_tz": -480
    },
    "id": "nAFMa-Tb1SLE",
    "outputId": "4dd40ba5-72f2-45b3-fc79-50404706a4e6",
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# hk_bgru_cp = ModelCheckpoint(\n",
    "#     filepath=hk_bgru_file_path,\n",
    "#     monitor=\"val_loss\",\n",
    "#     save_best_only=True,\n",
    "#     verbose=1,\n",
    "#     mode=\"min\"\n",
    "# )\n",
    "#\n",
    "#\n",
    "#\n",
    "# hk_bgru_model = None\n",
    "# # check if we have previously trained model or not, ref: https://stackoverflow.com/a/56425146/9500852\n",
    "# if os.path.exists(hk_bgru_file_path):\n",
    "#     print(\"Found existing model\")\n",
    "#     hk_bgru_model = load_model(hk_bgru_file_path)\n",
    "#     # score = hk_bgru_model.evaluate()\n",
    "# else:\n",
    "#     print(\"No existing model is found\")\n",
    "#     hk_bgru_model = make_bgru_model(\n",
    "#         input_dim=X_train_hk.shape[1],\n",
    "#         feature_cnt=X_train_hk.shape[2]\n",
    "#     )\n",
    "\n",
    "# start fitting the model\n",
    "# hk_bgru_history = None\n",
    "#\n",
    "# for i in range(len(fns_hk)):\n",
    "#     print(\"\\n--- Training {}, {} stocks remains ---\".format(fns_hk[i], len(fns_hk) - i))\n",
    "#     tmp_hk_bgru_history = hk_bgru_model.fit(\n",
    "#         x=X_train_hk[fns_hk[i]],\n",
    "#         y=y_train_hk[fns_hk[i]],\n",
    "#         validation_data=(X_test_hk[fns_hk[i]], y_test_hk[fns_hk[i]]),\n",
    "#         epochs=TRAIN_EPOCHS,\n",
    "#         callbacks=[\n",
    "#             hk_bgru_cp,\n",
    "#             hk_bgru_es\n",
    "#         ]\n",
    "#     )\n",
    "#\n",
    "#     # Append to previous histories\n",
    "#     if hk_bgru_history is None:\n",
    "#         hk_bgru_history = tmp_hk_bgru_history.history\n",
    "#     else:\n",
    "#         for dict_key in hk_bgru_history.keys():\n",
    "#             hk_bgru_history.update({\n",
    "#                 dict_key: hk_bgru_history[dict_key] + tmp_hk_bgru_history.history[dict_key]\n",
    "#             })\n",
    "#\n",
    "#     # Save model per fit() iteration\n",
    "#     hk_bgru_model.save(hk_bgru_file_path)\n",
    "#     print(\"After trained {}, Model Saved\".format(fns_hk[i]))\n",
    "#\n",
    "#     # Save history per fit() iteration\n",
    "#     np.save(hk_bgru_train_history_file_path, hk_bgru_history)\n",
    "#     print(\"After trained {}, History Saved, loss len: {}\".format(fns_hk[i], len(hk_bgru_history[\"loss\"])))\n",
    "\n",
    "    # SAVE HISTORY INDIVIUALLY PER fit() interation\n",
    "    # np.save(hk_bgru_train_history_file_path.format(HK_MODELS_CHECKPOINT_PATH, fns_hk[i]), hk_bgru_history.history)\n",
    "    # print(\"After trained {}, History Saved\".format(fns_hk[i])))\n",
    "\n",
    "# hk_bgru_history = hk_bgru_model.fit(\n",
    "#     x=X_train_hk,\n",
    "#     y=y_train_hk,\n",
    "#     validation_data=(X_test_hk, y_test_hk),\n",
    "#     epochs=TRAIN_EPOCHS,\n",
    "#     callbacks=[\n",
    "#         hk_bgru_cp,\n",
    "#         hk_bgru_es\n",
    "#     ]\n",
    "# )\n",
    "#\n",
    "# # save the model\n",
    "# if hk_bgru_model is not None:\n",
    "#     hk_bgru_model.save(hk_bgru_file_path)\n",
    "#     print(\"Model Saved\")\n",
    "#\n",
    "# # save the training history\n",
    "# if hk_bgru_history is not None:\n",
    "#     # np.save(hk_bgru_train_history_file_path, hk_bgru_history.history)\n",
    "#     np.save(hk_bgru_train_history_file_path, hk_bgru_history.history)\n",
    "#     print(\"History Saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "955EAavVAXC5"
   },
   "source": [
    "##### Train BGRU model for United States stocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 335,
     "status": "aborted",
     "timestamp": 1640615080053,
     "user": {
      "displayName": "Wilson Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12240555497150083704"
     },
     "user_tz": -480
    },
    "id": "-1YaYYh8AXC5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# us_bgru_cp = ModelCheckpoint(\n",
    "#     filepath=us_bgru_file_path,\n",
    "#     monitor=\"val_loss\",\n",
    "#     save_best_only=True,\n",
    "#     verbose=1,\n",
    "#     mode=\"min\"\n",
    "# )\n",
    "#\n",
    "# us_bgru_es = EarlyStopping(\n",
    "#     monitor=\"val_loss\",\n",
    "#     mode=\"min\",\n",
    "#     patience=5,\n",
    "#     restore_best_weights=True, # added to prevent overfitting in iterative fit()\n",
    "#     min_delta=0.0001, # https://stackoverflow.com/a/63495687/9500852\n",
    "# )\n",
    "#\n",
    "# us_bgru_model = None\n",
    "#\n",
    "# # check if we have previously trained model or not, ref: https://stackoverflow.com/a/56425146/9500852\n",
    "# if os.path.exists(us_bgru_file_path):\n",
    "#     print(\"Found existing model\")\n",
    "#     us_bgru_model = load_model(us_bgru_file_path)\n",
    "#\n",
    "# else:\n",
    "#     print(\"No existing model is found\")\n",
    "#     us_bgru_model = make_bgru_model(\n",
    "#         input_dim=X_train_us[fns_us[0]].shape[1],\n",
    "#         feature_cnt=X_train_us[fns_us[0]].shape[2]\n",
    "#     )\n",
    "#\n",
    "# # start fitting the model\n",
    "# us_bgru_history = None\n",
    "#\n",
    "# for i in range(len(fns_us)):\n",
    "#     print(\"\\n--- Training {}, {} stocks remains ---\".format(fns_us[i], len(fns_us) - i))\n",
    "#     tmp_us_bgru_history = us_bgru_model.fit(\n",
    "#         x=X_train_us[fns_us[i]],\n",
    "#         y=y_train_us[fns_us[i]],\n",
    "#         validation_data=(X_test_us[fns_us[i]], y_test_us[fns_us[i]]),\n",
    "#         epochs=TRAIN_EPOCHS,\n",
    "#         callbacks=[\n",
    "#             us_bgru_cp,\n",
    "#             us_bgru_es,\n",
    "#         ]\n",
    "#     )\n",
    "#\n",
    "#     # Append to previous histories\n",
    "#     if us_bgru_history is None:\n",
    "#         us_bgru_history = tmp_us_bgru_history.history\n",
    "#     else:\n",
    "#         for dict_key in us_bgru_history.keys():\n",
    "#             # us_bgru_history[dict_key] = us_bgru_history[dict_key] + tmp_us_bgru_history.history[dict_key]\n",
    "#             us_bgru_history.update({\n",
    "#                 dict_key: us_bgru_history[dict_key] + tmp_us_bgru_history.history[dict_key]\n",
    "#             })\n",
    "#\n",
    "#     # Save model per fit() iteration\n",
    "#     us_bgru_model.save(us_bgru_file_path)\n",
    "#     print(\"After trained {}, Model Saved\".format(fns_us[i]))\n",
    "#\n",
    "#     np.save(us_bgru_train_history_file_path, us_bgru_history)\n",
    "#     print(\"After trained {}, History Saved\".format(fns_us[i]))\n",
    "#\n",
    "# # save the model\n",
    "# if us_bgru_model is not None:\n",
    "#     us_bgru_model.save(us_bgru_file_path)\n",
    "#     print(\"Model Saved\")\n",
    "#\n",
    "# # save the training history\n",
    "# if us_bgru_history is not None:\n",
    "#     np.save(us_bgru_train_history_file_path, us_bgru_history)\n",
    "#     print(\"History Saved\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "executionInfo": {
     "elapsed": 10693,
     "status": "ok",
     "timestamp": 1640523438503,
     "user": {
      "displayName": "Wilson Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12240555497150083704"
     },
     "user_tz": -480
    },
    "id": "mbzVo16UGyqd",
    "outputId": "fa3f9468-6b4a-4cdc-83df-c01d723ab1a6",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### plot training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 336,
     "status": "aborted",
     "timestamp": 1640615080055,
     "user": {
      "displayName": "Wilson Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12240555497150083704"
     },
     "user_tz": -480
    },
    "id": "NFXjkTH-AXC5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# def plot_history(history_dict, title):\n",
    "#     \"\"\"\n",
    "#     Plot the training history\n",
    "#     :param history_dict: dict, the training history, should be a dict (from keras' history.history)\n",
    "#     :param title: str, plot title, example: \"HK BGRU Model - {}\", the program will replace the {} with the relevant metric name\n",
    "#     :return:\n",
    "#     \"\"\"\n",
    "#     metrics = [\"loss\",\n",
    "#            \"mean_absolute_error\",\n",
    "#            \"root_mean_squared_error\",\n",
    "#            # \"mean_absolute_percentage_error\" # disable plot of MAPE as the normalized data consists of 0 or nearly 0, the MAPE is unreasonably high, will recalculate in evaluation stage\n",
    "#            # \"val_loss\",\n",
    "#            # \"val_mean_absolute_error\",\n",
    "#            # \"val_root_mean_squared_error\",\n",
    "#            # \"val_mean_absolute_percentage_error\"\n",
    "#            ]\n",
    "#\n",
    "#     for metric in metrics:\n",
    "#         plt.figure(figsize=(14, 5), dpi=500, facecolor=\"white\")\n",
    "#         # metrics.replace(\"_\", \"\").title()\n",
    "#         plt.plot(history_dict[metric], label=\"Training\")\n",
    "#         plt.plot(history_dict[\"val_{}\".format(metric)], label=\"Validation\")\n",
    "#         plt.xlabel(\"Epochs\")\n",
    "#         plt.ylabel(metric.replace(\"_\", \" \").title())\n",
    "#         plt.title(title.format(metric.replace(\"_\", \" \").title()))\n",
    "#         plt.legend()\n",
    "#         create_dir_if_not_exist(MODEL_TRAIN_HISTORY_DIAGRAMS_PATH)\n",
    "#         # plt.savefig('{}/{}.png'.format(MODEL_TRAIN_HISTORY_DIAGRAMS_PATH, plt.gca().get_title()))\n",
    "#         plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rNGtylb0AXC5",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Plot HK BGRU training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "aborted",
     "timestamp": 1640615080055,
     "user": {
      "displayName": "Wilson Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12240555497150083704"
     },
     "user_tz": -480
    },
    "id": "FO5vMHRpAXC5",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# hk_bgru_history_dict = np.load(hk_bgru_train_history_file_path, allow_pickle=True).item()\n",
    "# plot_history(hk_bgru_history_dict, \"BGRU Model for HK Stock Price Predictions - {}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# hk_bgru_history_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e4yjS7-WAXC9"
   },
   "source": [
    "##### Plot US BGRU training history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1640615080056,
     "user": {
      "displayName": "Wilson Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12240555497150083704"
     },
     "user_tz": -480
    },
    "id": "l5TO3mbXAXC9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# us_bgru_history_dict = np.load(us_bgru_train_history_file_path, allow_pickle=True).item()\n",
    "# plot_history(us_bgru_history_dict, \"BGRU Model for US Stock Price Predictions - {}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-d0H4tvLAXC9",
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Other testing codes (to be removed)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1640615080056,
     "user": {
      "displayName": "Wilson Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12240555497150083704"
     },
     "user_tz": -480
    },
    "id": "nufgyJlZAXC9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Test train for one stock only\n",
    "\n",
    "# hk_bgru_cp = ModelCheckpoint(\n",
    "#     filepath=hk_bgru_file_path,\n",
    "#     monitor=\"val_loss\",\n",
    "#     save_best_only=True,\n",
    "#     verbose=1,\n",
    "#     mode=\"min\"\n",
    "# )\n",
    "#\n",
    "# hk_bgru_es = EarlyStopping(\n",
    "#     monitor=\"val_loss\",\n",
    "#     mode=\"min\",\n",
    "#     patience=10\n",
    "# )\n",
    "#\n",
    "# hk_bgru_model = None\n",
    "# # check if we have previously trained model or not, ref: https://stackoverflow.com/a/56425146/9500852\n",
    "# if os.path.exists(hk_bgru_file_path):\n",
    "#     print(\"Found existing model\")\n",
    "#     hk_bgru_model = load_model(hk_bgru_file_path)\n",
    "#     # score = hk_bgru_model.evaluate()\n",
    "# else:\n",
    "#     print(\"No existing model is found\")\n",
    "#     hk_bgru_model = make_bgru_model(\n",
    "#         input_dim=X_train_hk[fns_hk[0]].shape[1],\n",
    "#         feature_cnt=X_train_hk[fns_hk[0]].shape[2]\n",
    "#     )\n",
    "#\n",
    "# # from tensorflow.keras.layers import LSTM\n",
    "# # hk_bgru_model = Sequential()\n",
    "# # hk_bgru_model.add(Bidirectional(LSTM(units= 128), input_shape=(X_train_hk[fns_hk[0]].shape[1], X_train_hk[fns_hk[0]].shape[2])))\n",
    "# # hk_bgru_model.add(Dense(64))\n",
    "# # hk_bgru_model.add(Dense(units=1))\n",
    "# #\n",
    "# # hk_bgru_model.compile(optimizer=Adam(lr = 0.001), loss='mean_squared_error',\n",
    "# #                   metrics=[\n",
    "# #                     # MeanAbsoluteError(),\n",
    "# #                     # RootMeanSquaredError(),\n",
    "# #                     # MeanAbsolutePercentageError()\n",
    "# #                 ])\n",
    "#\n",
    "# # print(hk_bgru_model.summary())\n",
    "#\n",
    "# print(\"\\n--- Training {}, {} stocks remains ---\".format(\"1038.HK\", 0))\n",
    "# hk_bgru_history = hk_bgru_model.fit(\n",
    "#     x=X_train_hk[\"1038.HK\"],\n",
    "#     y=y_train_hk[\"1038.HK\"],\n",
    "#     validation_data=(X_test_hk[\"1038.HK\"], y_test_hk[\"1038.HK\"]),\n",
    "#     epochs=TRAIN_EPOCHS,\n",
    "#     # epochs=15,\n",
    "#     # batch_size=150,\n",
    "#     callbacks=[\n",
    "#         hk_bgru_cp,\n",
    "#         hk_bgru_es\n",
    "#     ],\n",
    "#     shuffle=False\n",
    "# )\n",
    "#\n",
    "# hk_bgru_model.save(hk_bgru_file_path)\n",
    "# print(\"After trained {}, Model Saved\".format(\"1038.HK\"))\n",
    "# np.save(hk_bgru_train_history_file_path, hk_bgru_history.history)\n",
    "# print(\"History Saved\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1640615080056,
     "user": {
      "displayName": "Wilson Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12240555497150083704"
     },
     "user_tz": -480
    },
    "id": "qvjHfLyeAXC9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# X_train_hk[fns_hk[0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 8,
     "status": "aborted",
     "timestamp": 1640615080056,
     "user": {
      "displayName": "Wilson Lee",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "12240555497150083704"
     },
     "user_tz": -480
    },
    "id": "YiTpuACwAXC9",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# hk_gru_history_dict = np.load(hk_bgru_train_history_file_path, allow_pickle=True).item()\n",
    "#\n",
    "# dict_key = \"loss\"\n",
    "# print(len(hk_gru_history_dict[\"loss\"]))\n",
    "#\n",
    "# # hk_bgru_history_dict[\"loss\"] = hk_gru_history_dict[\"loss\"] + hk_gru_history_dict[\"loss\"]\n",
    "# hk_bgru_history_dict.update({\n",
    "#     dict_key: hk_gru_history_dict[\"loss\"] + hk_gru_history_dict[\"loss\"]\n",
    "# })\n",
    "#\n",
    "# # print(\"updated\")\n",
    "#\n",
    "# print(len(hk_bgru_history_dict[\"loss\"]))\n",
    "#\n",
    "# print(hk_bgru_history_dict)\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "3.1. Model Training - BGRU.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
